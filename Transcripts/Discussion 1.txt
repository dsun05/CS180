Okay, there we go. So as I was saying, we will be recording these sessions. That said, attending live is like vastly preferred because broadly my plan will be to go over some material at the start, but hopefully this won't take too long, and then we can work on problems, you know, somewhat collaboratively, ask questions, discuss things. And I think this is super productive. So yeah, for this first time, I'm mostly going to go over some sort of review material, pre-reqs for the class, some of it, and then answer any questions. And yeah, I think that's all to say. So just before I get started, are there any questions on like logistics for the course or anything like this? Okay. Also, generally, feel free to interrupt me at any point. just unmute and ask a question if you have one. Okay, so today we're going to talk about a couple of sort of review items. So we'll discuss proof techniques, asymptotic notation, And then more briefly, Gale Shapley, which you saw during lecture. Okay, so everybody's taking classes in which they've sort of seen some basic proof techniques. This is very much a class where everything you do should be associated with proof. So generally in the class you'll be asked, you'll be given a problem specification, you'll be asked to design an algorithm for this problem, and then you'll have to prove properties of your algorithm. So Does it have the time complexity that you claim it has? Does it terminate? Sort of basic questions about algorithms are the things that you're going to want to be proving. So let's do a brief review of just some different ways of proving things. So to start, you just have sort of direct proofs. And this is basically you just show something in a very direct manner. So I'll give an example. So if the claim is that the sum of two rationals is rational. So by rational here I mean a rational number, so a number which can be written as a ratio of two integers. So you can prove this quite directly by saying let a over b and c over d be rationals, then consider their sum, and you can multiply the left-hand term and the right-hand term by d over d and b over b irrespectively to get something like this. So basically, you can write their sum as itself a rational, so as a ratio of two integers. This quantity itself is rational, since both AD plus BC and BD are themselves integers. So super quick example just to illustrate the point. A direct proof is where you can just sort of reason through something directly. So in general, you'll have some sort of assumptions or hypotheses like A, and you'll want to show that B follows and the proof will be of the form, assume A and show that B results and just do that linearly like that. Okay, there are other techniques which are less direct. So broadly they can be categorized as indirect proofs. One notable example that will come up a lot in this class is proof by contradiction. So let's do another simple example here as well. So first of all, before an example, what is a proof by contradiction? So you want to prove some statement P. Let's write this out. So to prove P, some propositional, some proposition, you instead assume not P, and then derive a contradiction. a contradiction just being any falsehood, anything that isn't true. So let's do an example. Let's show that root2 is irrational. Okay, so root2 is a number, it's a real number, and the claim is that it's irrational, meaning it can't be written as the ratio of two integers. So this I don't know a good way of proving, so to speak, directly, but there's a fairly simple proof by contradiction. So the first step is to assume for the sake of contradiction that root 2 is rational, right, the negation of what we want to show. So assume root 2 equals a over b for a b integers. And let's assume that a and b have been reduced. Like this fraction doesn't have any common factors in the top and the bottom. So assume root 2 is a over b for a b integers with no common factors. Oops. Okay. So then, what can we do? we can square both sides and learn that 2 is equal to a squared over b squared and so actually 2 b squared is equal to a squared and then from this what can we conclude? A is an even number? Yeah exactly so A is an even number so if A was odd then its square would be odd so therefore A actually must be even so A is even in other words A equals 2k for integer k. So what do we have now? We have that 2b squared is equal to 2k squared which is 4k squared. Let me write that out. So 2k the quantity squared which is 4k squared. We can divide both sides by 2 and get that now b squared is equal to 2 times k squared. And so now what do we get to conclude? That b is also even. Yeah, exactly. So b is also even. And now we celebrate because both a and b are purportedly even by this line of reasoning, but the very first thing that we did was assume that they have no common factors and if they're both even they share a common factor of two. So let's just say contradicting the initial assumption. Okay. So, super slick way of proving things. You want to prove some statement, assume that it's false, show that something which simply can't be the case follows, and therefore the original assumption was wrong. In other words, the original statement was actually true rather than false. Okay. I will pause at this point and see are there any questions so far on these two proof techniques? From this proof, can we generalize and say if a scroot of any integer is not an integer, then it is irrational? I'd have to think about it. Let's leave that as an exercise. That might just work. So you're asking whether square root k for integer k, if this isn't an integer, then is it necessarily irrational? Yes. So what happens? we get b so we get sorry k is a squared over b squared so b squared k is a squared so a is divisible by k. So a is divisible by k. So if a is divisible by k what can we say? We can write a as k times like l or m. Yeah exactly. So we get k say l squared. Should still be a k here. So we get b squared k is k squared l squared and so b squared is k l squared and so indeed b is also visible this only shows that sorry wait a second so if k is a factor here then k squared must be a factor of a squared yes i agree okay And so now b squared has k as a factor as well, so b squared must also have k squared as a factor, and so b has k as a factor. Yeah, yeah, yeah, same exact argument. Nice, yeah. In this case, like, we didn't use the fact that squared k is not integer. So if squared k is integer, this proof would not work. Yeah, exactly, yeah, because here you're only... Where is the part that we are, like, making sure that... Because, like, right now that I'm looking at this it shows like for all k but which is we know that it's not true like somewhere in the proof it should like show the part that like when k square is integer it's uh actually working it's only not working when the case it's uh yeah when square is integer it is integer it is rational yeah so this so if k isn't an integer then all we've shown is that a and b share this common non-integer factor but that doesn't contradict the initial assumption. So the initial assumption is that we're writing this root k as a ratio of two integers which are relatively prime which have no common factors no common integer factors right. So if we successfully at the end of this line of reasoning conclude that for k equals 3.781 whatever both a and b are divisible by that then it doesn't, there's no contradiction. The contradiction is exactly when k is an integer. Okay, all right, thank you. Yeah, yeah, does that answer your question? Yes. Yeah, cool. Yeah, nice question. So, yeah. Other questions? **JASON:** All right. Please do ask. **BEN:** What are you trying to show with b square equals to k times l square? **JASON:** Say that again? Yes, I'm just wondering what you're trying to show with b squared goes to k l squared that this like for anything. Yeah, so don't worry about too much. If you follow the root two is irrational, that illustrates the point sufficiently. Because we're just trying to get across like the broad point, this specific example doesn't really matter. But here the point is that k is a factor of b squared means that actually k squared is a factor of b squared. And so k is a factor of b. But again, like really don't worry if you're just, if you're worried about this example it's not important to the overall point. I see. Are you happy with the root two example? Yeah. Okay, then just be happy there. Yeah, that's the main point. Okay. So you will be writing a lot of proofs by contradiction in this class. You will assume something's false, derive a contradiction, and conclude that the original thing must have been true. Alright, let's do one more proof technique, namely induction. So in induction, you have a claim of the form p of n, where p is some proposition which depends on n. And you want to show that p of n is true for all n a natural number, this set just being 1, 2, 3, and so forth. So this is quite a powerful technique because you're essentially able to show like an infinite set of propositions rather than just a single proposition in some sense. So how do we do this? The approach is to first show that p of 1 is true. So this is called the base case. and then show that p of k implies p of k plus one for all k, at least that base case. So why does this work? So why does this work? Because if it's true for one, we know like next one should be also true, then it's true for two, and so on, like it should be true for all integers. Yeah, exactly, yeah. So if you've shown that p of one is true, you've also shown that p of one implies p of two, excuse me, you've also shown that p of two implies p of three, and so forth and so on. Yeah, exactly. Excuse me, I want to try to turn something off here. Okay, yeah, so that's the general technique. If this looks a little scary, let's give a concrete example. So Our claim will be that the sum of the first n positive integers is actually written as this simple formula n times n plus one over two. Okay, and we will now prove this by induction. So for the base case we just have to show that this is true when n is equal to one. So that's easy for n is equal to one. The left-hand side is just equal to the sum one. There's no other terms in it. And this is indeed equal to our expression, which is one times one plus one divided by two, which is one. Okay, so generally this is the case. The base case is sort of straightforward. You just have to check that it holds for a small number. But now the trick is to show this inductive step. p of k implies p of k plus one for all k, at least one. Okay, so how will we do this? Well, think back to direct proofs that we mentioned a second ago to show that a implies b. You first assume a and then you show that b follows. So in other words, we get to begin by assuming that this condition holds, which is quite a powerful assumption. So now assume that 1 plus 2 plus all the way up through k is equal to k times k plus 1 over 2 for some k at least 1. And now our goal is to show that just this assumption actually implies that the sum all the way up to k plus 1 also is equal to the claimed expression. So in particular consider the sum 1 plus 2 plus all the way up to k plus 1. And now we have to use the assumption that we've made and basic reasoning like algebraic manipulations to show that this is equal to what it's claimed to be equal to. So I'm just going to write here as our goal. We haven't actually shown this yet, but our goal will be to show that this expression is equal to what? Well, we have to plug in for n up here k plus one. So we're going to get k plus one times k plus one plus one, so k plus two, all over two. So we're considering this big summation and our goal is to show that it's equal to this expression and we get to use this inductive hypothesis, as it's called, this assumption here, in order to do so. Are there any questions at this point about the structure of the proof or what's going on? Okay and if not, does anybody have an idea for what the first step is? The first step is to use the assumption and apply it to 1 to the sum of k. Yeah, exactly. So just to write out exactly what the suggestion is, this sum 1 up through k+1 is actually just a sum all the way up to the number k, and then adding this additional k+1 one-th term, and so the suggestion is, well, for sums all the way up to k we already know a formula, so let's use that formula here. So we can replace this sum of 1 through k by the formula and not forget our extra term here, and so now this step follows by inductive hypothesis, induction hypothesis. The first step just follows by rewriting the sum nicely. And now we can just use some algebra to to finish up. So k times k plus one plus two times k plus one, now everything's over two. And then factor this, so k plus one times k plus two all over two. And indeed that was what we were trying to show it was equal to, and so end of proof, we've done it. Okay, questions about this proof by induction? So this of course was sort of like a simple example of using induction, but this technique, this structure of a proof, is extremely prevalent, like super powerful, used all the time, So, and indeed we'll be using that in this course to prove things about, for example, for loops. Okay, any questions? All right, so there's one other sort of quick proof technique-ish thing to be aware of, which are known as De Morgan's Law or De Morgan's Laws. So these are just useful things to remember on how to negate logical statements. So if you want to negate A or B, where A and B are logical variables, Boolean variables, true or falses. And have people seen this like "or" and and a negation notation before? - Yes. - Yes. - Yeah. - Okay. - I haven't seen the negation one. - Haven't seen the negation one. Okay. - It's just not? - Yeah, it's just not, exactly, yeah. - Okay. - There's lots of different things to write. Sometimes people write like X bar for the negation. Sometimes people write like not X for the negation. Programming languages, exclamation point X, but yeah, just a not. So if you want to negate the "or" of two things, so "a" or "b", well you can just rewrite this as "not a" and "not b". And then similarly if you want to negate "a" and "b", well this is actually equal to "not a" or "not b". You can think through why these are correct. It's not too hard to see this is right. And then relatedly it's useful to be able to negate statements that have quantifiers in them. So if you want to negate so suppose you have some statement there exists an object x such that you know some proposition that's a function of x there exists x such that p of x is true. Well the negation of this turns out to be the same as having the fact that for all x you have not p of x. And again it's worth thinking about why this is right, but it is. So it's not the case that there exists an x for which p of x. Well in other words for every single x not p of x. So these are the same. And then similarly for the dual case, so not of for all x p of x can be rewritten as there exists an x such that not p of x. So it's not the case that for every x you have p of x. Well in other words there exists some x for which p of x is not true. Okay, questions on these? Probably people have seen these before but good things to refresh your memory on. So, that concludes the little sort of recap or review of proof techniques. So, if there are any questions on those, let me know. Otherwise, we're going to move on to asymptotic notation. Okay, so this is a class about designing and analyzing algorithms. And generally we're going to be interested in the resources required by our algorithms, especially the worst-case resources. case in the sense of across all possible inputs what's the most of a resource the algorithm could use. And there are various resources that you can think about. So we'll think about time predominantly but also space. But it's useful as we talk about this just to think of these functions as describing time. And so how do you compare the runtime of two algorithms? Obviously you could just, you know, pick some inputs, start a stopwatch, run your algorithms on those inputs, and then see how long has elapsed. There are a lot of issues with just this sort of simple metric. It will depend a lot on, for example, the computer that you're using. It'll depend on which inputs you chose. And so what we'd like is a more sort of robust description of how the algorithm behaves and how long it takes. And the first useful step in building towards that is by looking at the runtime complexity as a function of the input length. So in general we'll design algorithms which can operate given arbitrary inputs, whatever the inputs are, whatever objects they are, they could be of all different sizes, sort of arbitrary sizes. And so then we'll think of the time complexity of an algorithm as not just being some number t but being some function t of n, where n is the length of the input in some some encoding. Okay, so that's the first step. We're going to think of the time complexity of algorithms as a function of the input length. And then the second step is to think about how fast this function grows in order to compare algorithms. So rather than write out the exact function, like say 100n or some other function, which is maybe n squared, instead of always writing exact function with all of the constants associated, we're just going to describe their asymptotic behavior. So what sort of behavior the function tends towards as n increases. Okay, so let's for this example, let's have a drawing of t and t prime. So maybe t prime of n is some sort of parabolic thing like this, and then t of n is some sort of linear thing like this? It's a pretty steep linear thing. The constant out front is 100. And this is just n squared. And so they intersect at some point. What's this point? 100? So how do you compare these two functions? It's not clear, right? Because for different values of n, one is larger, and for other values of n, the other is larger. So what we do is we look at what happens as n gets very large, and so then clearly in this case n squared is going to be much larger for sufficiently large n than just 100n, and indeed for any constant multiple of n, there will be some point at which n squared is much, much larger. And so that's exactly how we're going to define big O notation or asymptotic notation. So let's write this out. So for this example what we would end up having is the property that we say 100 n is in big O of n squared. Okay let's write this out a little bit more formally. So we'll say that f of n First let's just say that f and g are functions from natural numbers to positive real numbers. And we'll say f of n is in big O of g of n. if there exist constants c and n zero, such that for all sufficiently large n, in particular for all n at least n sub zero, we have f of n is at most c, this constant, times g of n. Okay, so this is exactly our definition. So in this case we do have that 100 n is in big O of n squared because there's this point n sub 0, 100, after which f is more than g, or sorry f is less than g, so 100 n is less than c n squared, where c in this case could just be 1. So there's many different combinations of constants that would work to show that something is in big O of something else but here's an example. Okay, questions on the definition? Okay So as some examples, let's have a and b be constants. Let's say that they're bigger than one, for example, and then bigger than zero. Then we have, and let's also assume that a is at most b, then we would have that n to the power a is in big O of n to the power b. Okay, so this is an example. So like n squared is in big O of n cubed. Also n cubed is in big O of n cubed. So these are all polynomials. These functions of n, they're just n to different powers. What are other examples? log n is in big O of n, and actually log to any power k, say k greater than equal to zero, is in big O of n. So log n to the hundredth n is still in big O of n. Also, so logarithmic functions are sort of smaller than asymptotically polynomial functions, and then polynomial functions, so n to the k, are in big O of 2 to the n. So again, sort of broadly, polynomial functions are in big O of exponential functions. Okay, so these are all just some quick examples. All right, any questions? So, big O, if you understand big O, then you basically understand all of asymptotics, but there are some other letters that are helpful. So, we also define F to be in big omega of G. You could define this again with sort of the existence of constants with the right property, but also you can just define it more simply if G is in big O of F. And then F is in theta of G. if both f is in O(g) and f is in O(g). So f being in O, big O(g) means that f grows slower than or at the same rate of g, roughly speaking, whereas f is in O(g) means that f grows faster than or at the same rate as g. So let's write that. So f grows faster or same as g, and then for big O it should have been f grows slower or same as g. And so these are sort of symmetric, these two. And then theta says both occur. So f grows same as g. These are in quotes, these are just sort of rough analogies. I mean, there are precise definitions in terms of these constants. But just for the sort of general intuitive understanding of what we're saying, this is exactly it. So if f is theta g, they're the same in terms of their asymptotic growth. Okay. There also exists two others. I'll write at least one. So we say that f is in little o of g if rather than there exists a constant c if instead for all constants c there exists this n sub 0 such that for all n at least n sub 0 we have f is at most cg. And so again following the sort of intuitive understanding of what this is saying, This says that f grows strictly slower, or let's just say slower, than g. So whereas big O left open the possibility that actually f is big theta of g, they grow at roughly the same rate, this now says that f grows strictly slower than g. So actually most of the examples where I wrote something is big O something else, the little o held. So indeed for both of these, you know, log n is in little o of n, and cubed is in little o of 2 to the n, and so forth. Whereas n squared is in big O, you know, 7n squared is in big O of n squared, but it's not the case that 7n squared is in little o of n squared. Okay. I have a question about f. A question about theta g. Would it be true if we say like if f is in theta g then it should be f is in big OG and also g is in big OF? Yes, exactly. Yeah, Yeah, exactly. Nice. So alternatively, we could have said, yeah, if f is in big O G, and f-- sorry, G is in big O F. And so actually, I should say, I'm being a little bit lazy here. You can just write the definitions of all of these in terms of precise statements like this. And there's lecture notes that do so. Plenty of lecture notes if you search asymptotic notation. There's like good cheat sheets for this stuff. But I wanted to get across the precise definition for big O and then give sort of the more intuitive definitions for the rest of them, because all the definitions are sort of the same in some sense. Yeah, what else to say about this? I mean, we'll use big O most. So why is that? Big O is giving an upper bound on a function, right? So you have some time complexity that you care about, t of n. And what you want to claim is that your algorithm is efficient. So you'll say something like, oh, well, t of n, the time complexity of my specific algorithm, is in big O of whatever it is, right? n log n, say. So you're claiming that up to constant factors and for sufficiently large n, the time complexity of your algorithm is bounded above by that constant times n log n. So in other words, big O gives upper bounds, whereas big omega gives lower bounds, and then big theta gives both at once. It gives sort of tight bounds. But this is all to say you'll use big O the most and the others somewhat less. There's also sort of a dual case to little o, which is little omega, which I won't bother defining, but it's defined in the analogous sort of dual way to little o. Okay, other questions on asymptotic notation? So for example, with big O notation, if f is an element of O(g(n)), that means that f is slower than g, right? Or sort of the same. One of these. But as seen from the asymptotics of 100n and n^2, at some time then, n^2 outgrows 100n. So do we look at before it grows above 100n or after it grows to over 100n, for instance? Yeah, so this is exactly pointing out that for sufficiently large n, so for n big enough, n squared is larger than 100n, or even a constant multiple of n squared is larger. And so what your question is getting at, so let's maybe draw this picture. In general, f is in O(g). Yes. The way that we defined this says absolutely nothing about the function's behavior before the point n sub 0 that we insist exists. So there has to be some point n sub 0 after which the function cg is sort of outpacing the function f. But before that we don't really care what do. So f could have been huge, it could have been oscillating, it could have done anything, and g could have been, you know, tiny, whatever. But to say that f is in big O of g, all we mean is that there's some constant multiple c, such that cg is bigger than f for n big enough. Does that answer your question? Okay, so we're looking at, like, we're looking at a time when like n square for instance it goes above like 100n then that means that 100n is an element of big O of n square. Exactly. Okay, thank you. Yep, absolutely. Yeah, and I should point out that we're using this in or element of notation. And so O of n squared is the set of all functions that have this property. Okay. Can we have a more complex expression inside the O of n, or O of n squared plus 7 or something like that? Yeah, so you can define this at sort of different levels of generality, but here a reasonable one here, or what I was thinking was f and g are just any functions from natural numbers to positive reals. And so then you can define the class O of f for any function f. Okay, thank you. Yeah. You'll sort of get used to it. So there are like specific functions that we'll see a lot of. So of a brute force algorithm that does something in time two to the n. And then the big question is, can you make it down to anything polynomial like n squared or n cubed? And then also there'll be cases where there's sort of a straightforward algorithm that already achieves polynomial time like n cubed or n squared. But if you're clever, you can get it much lower to something like n or n log n. But the point to stress here is that as I say these growth rates, I'm just talking about n cubed or n log n. I'm not saying like 7n log n. We just sort of ignore the constants by using this asymptotic notation. Which is nice for a number of reasons. One thing it does is it makes the time complexities of our algorithms somewhat robust to the particular model of computation that we use. So there's different ways to formalize a model of computation. The lecture notes provide one way, but there's many others, but in general you can simulate different model of computation with each other up to constant factors, sometimes polynomial factors, and so the asymptotic notation sort of lets us hide some of those details. Okay, other questions? Okay. So the last thing, which is what you spent a lot of time during lecture this week doing, getting into our first algorithmic problem and our first algorithm for a problem. So specifically the Gale-Shapley algorithm for matching hospitals and students. This is a nice problem for a number of reasons. One of them is that a variant of the Gale-Shapley algorithm that we will discuss here, that you've already discussed in lecture, is actually used to match students and hospitals for residency, for matching doctors to hospitals for their residencies. And so this actual big matching algorithm actually gets run every year and is used to make decisions. And so being able to analyze its properties and potentially design algorithms with nicer properties is quite applicable in an interesting case. So let's first define-- let's just review, because you've seen this in lecture. Let's first review the problem setup, a couple of definitions, and then we'll quickly go over the algorithm of its properties. Okay so the Gale-Shapley problem is as follows. So you have n hospitals, h1 through hn. You also have n students, doctors, people, call them s1 through sm. And then for each one you have a ranking. So each hospital, each HI, has a ranking of all students. So what I mean by this is hospital HI might prefer student 5 the most, student 7 second most, student one third most and so forth until they they really dislike student two whatever. So every hospital has such a ranking and then similarly each student has a ranking of all hospitals. Okay so you have these two end rankings and then the question is how can we match them up? So a matching is a set of pairs coming from H cross S. So a matching M is some set of pairs, each of the form some hospital and some student. So this notation means-- If you haven't seen this, this just means all pairs H,S, where H is a hospital and S is a student. I should say up here, I'm calling this set H and I'm calling this set S. And a matching is a set of such pairs such that each hospital is in at most one pair. And same for the students. So each student is in at most one parent. OK, so that's a matching. And then a perfect matching, again, is some set of pairs from h cross s, such that same as the above, each h in H. But now instead of each hospital needing to be in at most one pair, it's in exactly one pair. So each hospital is in exactly one pair. You know. And similarly, so each student is in exactly one pair now. Okay. So as an example, we have this set of students. We have this set of hospitals. I should draw them the other way. So a matching says something like this. I've matched up some of the hospitals to the students. And now a perfect matching is one such that everybody gets a match. Okay. So a couple more definitions and then we're done with that. We can go over the algorithm. So a perfect matching M is said to be instable or has an instability if there exists the following. So there are some pair HS and some other pair H prime S prime in this perfect matching M. such that unfortunately H prefers S' so even though H got matched to S, H would rather be with S' and S' also prefers H. So even though S' got matched with H', S' actually would rather be with H. So why is this an instability? because if the matching algorithm tried to pair up the hospitals and the students according to their rankings, and then there was an instable pair like this, H and S prime, there's nothing stopping them from just sort of aborting from what they were assigned and instead pairing up with each other. So this would be instable because then we've left H prime and S unmatched. And maybe they don't want to be together. Who knows? OK, so this is an instability. So what is obviously desirable is to come up with a perfect matching which is stable in the sense that it has no instability. So let's write that a perfect matching M is stable if it has no instability. Instabilities. Okay. So not all algorithm problems will be like this. This required a fair bit of definition, but now we're ready to sort of state the problem. So the input to our problem will be the set H, the set S, and their rankings. And our output, or our goal, let's say, is to find a stable matching for this input. And I should say if it exists, because a priori, it's not clear that such an object even exists. It's not hard to see that a matching exists, right? You could just take the first hospital and the first student and match them, and then the second and the second and match them, and so forth and so on. and so that would generate a perfect matching. But to get this stability property that there are no sort of pairs of people who would rather break off and form a new pair, it's less clear whether this exists. Okay, at this point we've stated the problem and we've defined several things. Are there questions about any of it? All right. If not, let's state the famous Gale-Shapley algorithm now. So recall that its input is just h, s, and the rankings. And h and s have the same size, size n. So how does the algorithm go? We start with our matching that we're going to construct being empty, the empty set. And then we're going to go through a loop. So while some hospital is not matched, meaning there is no pair containing H in M yet, we'll do the following. So we have this hospital H, which is not matched. We'll let S be the first student on the ranking of H. that is not yet offered to. Actually, let's say this. That is not crossed out. Okay, so let S be the first student, sort of the highest ranked student by h, which has not yet been crossed out. First of all, if s is unmatched-- so s is not already matched with somebody-- then match them. So then add h and s as a new pair to our matching m. Otherwise, do the following. So otherwise, let h prime be the current match of s. That is, h prime comma s is in the matching right now. So now, if s prefers h prime to s, what order do I want to do this in? So let's first do the other one. So if S prefers H, then remove H prime S, the current pair, and add the new pair, HS, to M. So in other words, if S likes this new offer that it's receiving from H, then go ahead and update the matching accordingly. And note that H prime is now some hospital which is not currently matched. And so this will sort of be something to do in a future loop. Otherwise, do nothing. So else s rejects the offer from h, so to speak. And then finally, let's say that h crosses out S from their ranking, from their preference list. So that is to say, H will never end up making an offer to S again. H is done with them. And next time through the loop for H, we'll say let S be the first student on the ranking not crossed out. So it'll be somebody down the line. OK. And then finally, at the end, return the matching that you get. All right, questions on the statement of the algorithm? So this is a nice algorithm. Has some good properties. So let's write a couple of them. Now, a good chunk of time was already spent on this in class, so I'm not going to recreate all of that, but I'm just going to go over a couple of the main ones. And then there's some more on the homework as well. So first of all, let's show that Gale-Shapley terminates. And in fact, it takes at most n squared steps, or iterations of the loop. And let's write iterations. Anybody see a reason that that's the case, a way to prove that? I should say this is like a very sort of common question that after you've designed an algorithm of something that you want to show, right? So right now we have this sort of unbounded loop, this while, which depends on a condition. So is it the case that that loop can just run infinitely? That would be terrible. Instead we're claiming that no, finitely many steps, in fact, quadratically many steps. Anybody see why? Yeah, go for it. - Maybe just intuitively, we know that there's n hospitals in the same n students. And once we check something once, we don't check again. So there's at most n squared. - Yeah, yeah, exactly. So how does the algorithm proceed? Well, in every iteration through the loop, somebody gets crossed out, right? This h crosses out s from their ranking happens every time. And so the, you know, We will have crossed out all n squared steps, all n squared positions in the rankings by the time-- we will cross-- there are at most n squared iterations that will occur. It could be the case that fewer could occur. Like the hospitals could all end up being matched in many fewer than n squared iterations. But it will take at most n squared iterations. So yeah. I'm not going to write this all out, but okay. So this is a first example of something we can say. One question that's worth thinking about is are n squared iterations necessary in general? So can you come up with inputs where all n squared iterations would be used? Or is it the case that actually always you'll have a stable matching after only say n iterations or n log n iterations? So let's say are n squared iterations ever necessary? Okay. Okay. Another claim which I'm not going to show here because a lot of time was spent on this in class, but the main claim is that Gale Shapley returns a stable matching. And actually you can say a lot more than this. So it returns like a stable matching of a particular form with specific properties among possibly multiple stable matchings that don't all have those properties. But this is sort of the first basic fact. And then, so proof in class. And then what's the final claim that I wanted to say? Yeah, so a consequence of this is that a stable matching always exists, which is a nice claim. So no matter what preference rankings you come up with, you will always be able to find a stable matching. Why is this the case? It's just by the above two claims, right? So if it's the case that Gale-Shapley always terminates, and then when it terminates, it returns a stable matching, then why does a stable matching always exist? Well, just run Gale-Shapley, and the output is a stable matching, thus it exists, so. So, nice consequence of that. Okay, so this is sort of the first algorithm which we've seen in this class. Kind of a classic algorithm, nice algorithm. And then we did a little bit of asymptotics and sort of proof technique review. So are there any questions about anything we've discussed so far? So the Gale-Shapley algorithm always returns a stable matching, but it doesn't always return a perfect matching, right? That's a good question. So a stable matching is also perfect. It's also perfect. I meant to define it this way. Yeah, so the word stable only applies to perfect matchings. So it will be perfect as well. Yeah. - I see. - Yeah. - Are they not different? So they're both the same? - So they're not the same. Like you could have a matching, which is, you're right. This is like a natural question. 'Cause you could have a matching, which is stable in the sense that it has no instabilities. We defined it, I guess, so that the word stable only applies to perfect matchings to begin with. But you're right, so they're different concepts. So perfect just means that everybody ends up getting matched, and then stable means that you don't have any of these pairs that would rather be with each other. And indeed, Gale Shapley will return something with both of these properties, that everybody gets matched and you won't have anybody who would rather be with each other. OK. Yeah. Other questions? I realize we didn't go through a lot of the proofs here, but this was covered pretty rigorously in class already, and there's good notes on it. And I wanted to go over some of this asymptotics and proof technique stuff as well. Yeah, other questions? (The End