All right, welcome everyone back to week two. So just a couple announcements. Homework one was released last week. It will be due this Saturday. This will be covering stuff basically through today. Big O notation as well as scale shapely algorithm. As was mentioned in the syllabus for the homework assignments, three of the problems will be graded based on correctness and the remaining problems will be based on completeness, which means you just need to show a good attempt for the problem to get full credit on those ones. As far as the algorithm type questions, so if you're asked to write an algorithm, then you have to prove its correctness and also generally give a time complexity analysis, unless otherwise stated. So in terms of the level of rigor required for these things, for the correctness proof, this should be if you take someone in this class, just kind of average person in this class, then it should be a convincing proof. Like if you gave them this proof, you explained it to them, then they should be convinced that the thing is true. So for example, with the Gale-Shapley algorithm, where we said that it outputs a stable matching, that's not obvious just by looking at the algorithm itself. It requires explanation, where certain observational things such as that the hospitals propose or give offers in decreasing order and preference, that can be kind of read off just from the algorithm. So when you're wondering, do I have to prove something? Do I not have to prove something? Just imagine you're kind of a reasonable person in this class who doesn't want to take a lot of effort and look at the then that's fine, you don't necessarily have to prove that. But if it's something that is not clear on first viewing, then that's something you should prove. As far as the time complexity analysis, we'll cover that today. But what you want to do is have something where a reasonable person in this class could actually implement ENCODE without too much effort. So if you're looking over the algorithm-- and this will include talking about data structures-- but if you're looking over your algorithm and you're wondering, huh, we have this kind of notion, they pick this thing and it's unclear exactly how you would implement that in terms of time complexity. How long would that step take? Then that's something you should discuss in further detail. And so a lot of times what this can be is you can just write down your high level algorithm and then have little notes saying, okay, for this I'll use an array or this I'll use a queue or something. And you can kind of talk about the time complexity as it goes. So if you have a nested for loop, you could just say that you could multiply the iterations of the for loop together. That's fine. But yeah, if you have a step, which we'll see later, and you're kind of unsure of-- without taking much effort, how actually would I run this part of the algorithm, then that's something you should explain in a little bit more detail when talking about the time complexity analysis. And again, we'll do an example with the scale shapely algorithm today in lecture. some other announcements. So just in terms of resources, so the lecture notes and the recordings are posted after lecture, generally in the afternoon afterwards. Additionally, I have also posted, and I will be posting further notes on this class. So those ones are kind of additional supplementary material, they summarize some of the more important concepts in the textbook, then those can be used, for example, if there's a proof in class, maybe you want to see it written down in more detail. Those will be in those notes. However, they are a little bit different from what you'll see in lecture, as those generally don't have as many examples or diagrams or things. So those are more kind of a skeleton outline of some of the concepts from this class. I've also posted just today a link to textbook slides that were made for the textbook we're using. So when the textbook was released, it was also released with lecture slides. I am not using those lecture slides, But if you want to view those, I put a link on it for week one. So you can go through and look at those. Those also have additional details. So just other resources for you guys to have. Of course, if you want to practice algorithms problems, there's a lot of examples in textbooks. Some of those will be on your homework. And then you can also Google problems. You can find some on LeetCode. There's all sorts of places to find resources. But yeah. So those are the announcements. Any questions on the content of this class so far, what we've covered, or just on course logistics? OK. So let me share my screen at this point. All right, so we're gonna pick off with what we talked about for Big O notation, which is where we're analyzing the efficiency of algorithms. So generally speaking for this class, you only need to know Big O notation insofar as you will need it to analyze your own algorithms. So there won't be any questions on the exam. I think there were a few questions on the homework, but you won't be required to prove that say one function is Big O of another function, or one function is Theta of another function. Those types of things will not be tested. You just need to know how to use this so that you can actually talk about how efficient your own algorithms are. OK. So previously, we mentioned that when we were talking about efficiency, we would like some notion that's a little bit independent of the actual architecture you're working on, as well as something that allows for some flexibility when we're talking about what is a basic step of computation. So we define the runtime of the algorithm to be the number of basic steps it takes for the algorithm to run, where a basic step is kind of some primitive operation, like a multiplication or addition, or say, accessing an array element. Now, this is kind of a loose definition, but the actual efficiency notion we will allow for some looseness. So as long as you don't think something takes something proportional to the size of the input to run, if something takes three steps versus one step, our notion of efficiency will allow for this and not cause us to have to worry about that. So the general one we'll use most in this class will be big O notation. So here's a definition for big O. we say that f(n) is big-oh of g(n) if there exist constants c bigger than 0 and n not bigger than or equal to zero, such that for all large enough n, so for all n that are at least n zero, f of n is less than or equal to c times g of n. So as a diagram, this will look something like the following. You might have some f of n, and you might have, say, some g of n. Yeah. I'll draw a better one. Okay, so this is c times g of n, and this is f of n. And at some point, in this diagram here, this is n and this is runtime. And here is the input length, generally speaking, of the algorithm. So as your algorithm is run on larger and larger inputs, eventually the runtime is upper-bounded by c times g of n. So generally speaking, when we do this, f of n will be the algorithm runtime, and g g of n will be an upper bound on the runtime. So what we're really saying here is that the algorithm is upper bounded. So if the algorithm runtime is f of n, where n is the input length, then the time it takes for the algorithm to run is upper bounded by some function g of n. If when the input length gets large enough, it's always less than g of n, times some constant, where this constant allows us to, say, have flexibility in saying something is two steps versus one input step. We're just going to say we don't actually worry about the constant in front of the most important term. OK, so as an example, let's black. We can say that-- so let's say the runtime-- Let's do an example here. So for i equals 0, i less than n, i plus plus. And then we'll have another for loop. j equals 0, j less than n, j plus plus. Maybe we'll do printf. Hello? This, and we want to say, how long does this code snippet take to run? So how many steps? So just here we're going to allow that printing will say it takes one time. So constant time. So we'll say this is constant time. So we call this O(1), that means it takes one, two, or three steps, for example, or five. And we want to know what, how many steps roughly does this take? Or what's an upper bound on a number of steps? Any ideas for this? N squared? N squared, yes. Okay, so how did we get N squared? So this is, so the time, runtime, t of n here. Okay, we can imagine n here being the input length, is O of n squared. And why is this true? Well, we can think is that, okay, what's the actual, so let's do some more analysis. So the number of printf statements. So yeah, if we wanted to go into this in more detail, we can say that we're going to have, so the number of printf statements is basically n squared, right? N times n is n squared. And we have the number of update operations. This is j++ or i++. This is going to be also roughly n squared, or yeah, so this will be n squared plus n. This n squared is for the j++, and this n is for i++. And then the number of kind of, you know, check conditions. So this is say, i less than j less than n, or j less than n. This is also n squared plus n. And then also we have the initial assignment of i and j to zero. So initial assignments of i equals 0 or j equals 0, this is n plus 1. So n times for j and 1 times for i, n in total. Let me zoom out a little bit. This is going to be something like approximately, It's approximately, it's kind of 3n squared plus 3n plus 1 operations. Okay, now this type of analysis I'm doing here, all this stuff, this is completely unnecessary when it comes to your homework and exams. If you just write this, that's totally sufficient for full credit. So let me just write that down. this analysis is unnecessary for homework, because for exams, because this is very easy to look at and say, it's n squared operations. But if you want to look through this more in depth, you could dig down and say, okay, what is actually happening? each of these we consider a primitive step. But maybe you think, OK, checking i less than n, maybe that should actually do two steps rather than one step. So maybe you would think this should be 2n squared plus 2n versus just n squared plus n. Or maybe you think that the checking and the addition here should count as one step. So we have kind of this flexibility of what's one step, what's two steps, what's three steps. But the major point here is that the value we actually care about is this n squared, this term that's going to be growing. So 3n squared plus n plus 3n plus 1 is O(n) squared because 3n squared plus 3n plus 1 is less than or equal to, say, I'll just give the big number, we'll just say 10n^2 for all n, I believe at least one should be fine. Let's see, it would be 3 plus 3. So here you could that this is the constant C, and this one here is n zero. So we're saying for all large enough n, our runtime is bounded by some constant times the actual runtime, n squared, that we care about. And we're allowing for this kind of slack in the runtime constant here to account for say, you think that maybe these don't count as a step, or maybe these count as three steps or two steps, that kind of stuff. So that's kind of what's going on under the hood, why we have this definition, is that we want to really care about, what happens as the runtime increases, right? If you add another for loop, then it's going to become n cubed, and as n gets very large, that will actually have a dramatic increase on the runtime. Whereas these constants here tend to be more dependent on the actual implementation and hardware and those types of things. and we want to ignore that for the time being. Now in practice, does this constant matter? Of course it does. If it's constant is 12 billion, then generally you'll be unhappy. But in practice, it kind of works out to use this as a pretty decent upper bound. These constants do tend to be small most of the time, or small enough. But yeah, for the purposes of your homework and exams, this analysis here is fairly simple. So it would be enough to just look at this and say it's n squared and not do any of this analysis. Later down the line, we'll run into things where it's a little bit more complicated to do the analysis, in which case you need more explanation. any questions on this. Harry, do you have a question? OK. All right, so. some nice properties about order notation. So again, recall what is it? It just says that your run time for large enough n, for large enough input lengths, your run time is upper bounded by a constant times the thing that's in the big O notation. So there are some nice properties. I'm not going to prove them all here. So, oh yeah, one thing on notation. So, technically the correct way to say is that f of n is an element of O of g of n. So what does this mean? This is an element of... And O of g of n is a set of functions. It's all the functions that are upper bounded kind of in the way that we defined. by g of n. So this is the correct way to say it, but as shorthand, you often see this as following. See this as f of n equals o of g of n. Alright, so usually people don't write this element of, they just write equals. This is very common, you can do this as well if you want, but just keep in mind that this is sort of not equality in the same sense. So while 5n squared is O(n) squared, and 3(n) squared is also O(n) squared, but 5n squared is not equal to 3n squared. So this is really a more notational shorthand than it is actual equality. So just keep in mind that you can write it like this, that's totally fine, but it's like a one-sided equality. It actually should be that this function is an element of this set of functions, because if you get the equality through, you would think that this is true and that's not true. And I will probably use either of these interchangeably. you're welcome to use them interchangeably. This is actually more common to see, just because it's kind of annoying to write this element of symbol. Element of, by the way, is just kind of like a C with a line through it. Okay. So some useful properties about this. So, you know, function is upper bounded by itself. That makes sense, right? function is always at least as big as itself. If you have f of n, if a function is upper bounded by another function, so f of n is-- you should think of big O as kind of less than or equal to g of n. And if you have a constant c bigger than 0, then c times f of n is also upper bounded by g of n. So here's where I mentioned before, this definition allows us to kind of ignore the constant in front, because when we want to prove that something is bigger of something else, we can always make the other constant much, much bigger. So this allows us to not have to worry about the hardware changes of something taking three steps versus two steps or five steps There we go. Okay. You can also do kind of addition and multiplication. So if f1 of n is order of g1 of n, so if f1 is upper bounded by g1 of n, and f2 is upper bounded by g2, then the functions are based, the addition of the functions is going to be upper bounded by the maximum of these two. Then f1(n) + f2(n) is upper bounded by what's called the max of g1, g2. Okay, so what is the max? The max here of g1, g2 is whichever one is bigger. So g1 of n, if g1 of n is bigger than or equal to g2 of n, or g2 of n else. So as drawing as a picture, if we have, say, maybe this is G1 and this is G2, then the max is just you go over the top. So the max would just be this. So this is max G1, G2. So you always just take what's on top. OK, I'll add some highlighting here. So this is kind of nice when it just says that for addition, you just take whichever one is the bigger upper bound. So that's pretty nice. And this extends if you have multiple functions. So if you're adding, if you're like-- and now, when do you get addition? you get addition when you run one algorithm and then run another algorithm. So if you say, OK, this part takes 10 steps, this part takes 20 steps, then the addition, kind of the max of each part. So going back to our example before, I said, well, this part takes n squared. This part also takes n squared plus n. This part takes n squared plus n. Another way you can think of as a total runtime is adding all these functions up. and each of these parts themselves are n squared, order n squared, which means that the final thing is also order n squared. So that's where you get addition. Addition is when you kind of break your algorithm up piecewise into parts, and then you just have to look at the runtime really of the most expensive parts. It will be dominated by that. Okay. Any questions so far on this? And I won't be proving them here, but the proofs are in my additional notes as well as in the textbook. Okay Two more properties that are probably pretty useful to have so multiplication so F1 of n is order G1 of n and is order g2 of n, then f1 of n times f2 of n is order g1 of n times g2 of n. So multiplication, you can just multiply the upper bounds together, which is really nice. Okay, so where is multiplication? Do additional notes cover more than the lecture? Yeah, so basically what you do need to know for this class is what's covered in the lecture. The additional notes I posted on brew and learn have some extra proofs and some extra supplemental stuff you can look at, but you'll primarily be tested on what's covered actually in lecture. Oh, yeah. Oh, yeah, someone asked if printf is a basic step from before. Sorry, I missed that. Yeah, so printf, we kind of were saying that we're going to say like basic, you know, how many steps will that actually take? It's actually a function call. Here we're going to be considering it to be one step, but it probably would take some constant number of steps. The important part is that printf here, it doesn't actually depend on n. So however many steps this takes, maybe it takes a hundred steps, it's going to be a constant. And so our big O notation lets us ignore how big the constant is. But yeah, good question. Okay, so back to this. Yeah, where do we get multiplication? Multiplication comes when, say, you run one algorithm many times. So imagine, so this often occurs as follows. This is like, let's say F1 is a part of code. And this is how many times you run it. You run that code. So if we go back to our example from before, we can see that this is like constant times, so some constant, we'll call that O of one. And then why do you have multiplication? Well, because we ran this part n times. So it's n times O of one, which is O of n. And then we ran this entire segment another n times. So this was n times the size of this thing, which was O of n. So that's where we got O of n squared. So again, additions, how it's happening when you break it up piecewise, multiplication usually occurs with kind of these for loops, or when you have a piece of code that you're running multiple times. And for there, for big O notation, it's really nice because you can just multiply the two upper bounds together. Okay. Any other questions on this so far? All right. And then the last one that will be useful for us will be, well, there's some more properties you can find in the textbook as well. But if f of n is upper bounded by g of n, and g of n is upper bounded by another function h of n, then as you would expect, f of n is also upper bounded by h of n. So this upper bound here does not have to be a tight one, but this is kind of a transitive property, right? So if this is at least, is smaller eventually than this, and this is smaller eventually than constant times this, then F is smaller than the biggest one, the constant times the biggest one, which is H of M. Okay. All right, so let's kind of talk about some common expressions and what you'll see for these big notation. So, common expressions. All right, from smallest to largest. All right, so the smallest one that's usually covered is O(1). O(1) is constant time. And that's because we can say it's smaller than or equal to c times g, which is g(1). So this is smaller than or equal to c times 1 for some constant c. So this means your runtime is constant time. As in, the entire runtime takes less than or equal some constant after some big enough input. Then we have nested logarithms. So if you have logarithms nesting each other, you probably won't have this much during this class. I would not expect. Then you can have powers of logarithms. So this is for some constant, epsilon between 0 and 1. So this could be, say, for example, this could be square root of epsilon. This could be cube root of epsilon. So like root epsilon is bigger than log log. sorry, square root of log n is bigger than log log n. Then we just have the basic logarithmic time, log n. Okay, and then you can have powers of log. So this is for constant, c bigger than or equal to 1. This is also called polylogarithmic. So this is log squared, log cubed, log to the fourth, and so on and so forth. The next one you often see. And this is not a comprehensive list. These are just common ones you might see if you're doing analysis. Here it will be n to a small constant. This is for a constant. Zero less than epsilon less than one. So this is, for example, square root of n, cube root of n, fourth root of n, those things. Once you get to powers of n, you're bigger than the logarithmic regime, generally speaking. Then we have O of n. This is also known as linear time. So generally speaking, if you have to at least read your input to know what the answer should be, then your algorithm must be at least linear time. So something is less than linear time, that means that you don't even have to read your entire input to know what the answer should be. So for example, for a binary search, if you kind of like, you know, check the middle, say it's a bigger or smaller, and then you go to another half, you don't actually have to look at your entire list of values if they're already sorted to figure out where your item is. However, if you're, for example, have an entire list of numbers and you want to sort them and they're not sorted already, you have to at least look at every number in the list to sort it. So you should expect the runtime to be at least linear for sorting ends up being n log n. But for linear time, you should really think the cutoff point between less than linear and more than linear is whether or not you actually have to read every part of the input to know whether the answer should be something or something else. Okay. Another one you'll see is n log n. This you see very commonly with sorting algorithms and also heard of like recursive divide and conquer type which we'll talk about later. Then we have, and this is e for a constant, c greater than or equal to one. This is generally called polynomial time. Well, technically polynomial time covers, polynomial covers this and everything below it, since this is an upper bound. Then we have two to the n. This is often called exponential time. Turns out actually a lot of things are called exponential time. As soon as you have an n in the exponent, that's usually called exponential time. So two to the n, three to the n, four to the n, these are different and they're not actually like, you know, some are a lot bigger than others, but we tend to call them all exponential time because the reason is that once n is in the exponent, these run times tend to be too slow to actually ever run practically. Like for example, if n is say even like 60 or something, no one's going to be running the algorithm. It's much too slow and we won't be using it. So exponential time is often used colloquially as kind of a catch-all for everything with n in the exponent because we generally don't want to run any of these. And generally these are similar to the brute force time for the algorithm. Okay, but you can get bigger than exponential, you can get factorial, you can also get, for example, n to the n. And note that some laws, right, if you have n to the n, this is two, so log base two of n to the n, which you can simplify as 2 to the n log n. So, yeah, another way to kind of compare them. Okay. So yeah, so here's some common expressions. When you're doing for the homework, there's a problem where you have to compare them. You're not required to do any proofs. You just have to say how the comparison works. And when you're trying to remember how to compare things, now what will you do in practice? What you'll do in practice most of the time is that you'll get kind of a hang for this, but generally speaking, a lot of times, you can kind of see how the growth rate looks, or you'll Google it. For the homework, please don't just Google the answers. That's not allowed, but you can kind of use a lot of the rules of exponents and logarithms. So this rule here, where 2 to the log base 2 of something is the same thing about something, that's a useful one, or the change of basis formula for logarithms is also useful. So if you're trying to kind of think about how do I analyze these a little bit, you can try to change them into different expressions that look closer to other expressions using rules of exponents and logarithms. Okay. - Cool question. - And yeah. - Does it make sense to talk about something in terms of big O if the function itself like approaching zero, for example, does it make sense to talk about like O of one half to the power of n? Oh I see, okay, so you're asking what if you have something that's one half to the n, like this? Yeah, like does it make sense to talk about big O in that sense in any kind of way? Yeah, yeah, so okay, yeah, good question. So the question was like what if you have something like like this, where if you look at the function here, it's going to very rapidly approach 0. So this is a valid definition you could use, but it's uncommon to use this. So probably what would happen in this case is that you would just say that the algorithm is-- well, it's a little odd to think how you have an algorithm whose runtime approaches 0. But suppose you did have one bit like this. Usually, people just go to constant time as the lowest one. So probably you would say, OK, well, at some point, it becomes less than some constant. This is kind of the lowest people tend to use for an upper bound. And you would likely use this rather than this kind of more strange expression. But technically speaking, this is something you could use. It's just not common to do that. - Okay, thank you. - Yeah. - Shouldn't the function inside the big O be always increasing? 'Cause like the more N you have, it should like be more time to push, like do it. Shouldn't always be-- - This function be increasing? - Yeah. - Oh yeah, these are increasing. Or is there one that's not increasing here? - There's a one over two to the N is gonna be non-increasing. Oh, this is just an aside. It's not part of the list. This would be, I guess, over there. This was just to answer the question that someone asked. This is not part of the common list elements. So I'll move to the side. OK, thank you. OK, all of these here should be increasing the thing inside as I pertain them. Okay, so another thing we'll define that is what we call efficient. So generally speaking, we refer to efficient as being polynomial time. So that's n to the c or less. So let me define that. So, okay, I'm going to erase this for now. So polynomial time, you say that an algorithm is polynomial time if there exists some constant such that the runtime T of n of the algorithm is O of n to the c. So this means, for example, if it was n squared, n cubed, n to the fourth, those are all considered to be polynomial time. So basically you should think the run time is a polynomial function of the input length. Where a polynomial, if you recall, is, for example, 5n^2 + 2n is a polynomial, or 12n^3 + 3 is a polynomial. So the definition of polynomial time is basically here and below. All right. And the reason why we define polynomial time is because that's also how we're going to talk about efficiency in theoretical computer science. So we say, is there any situation where runtime decreases while n increases in practice? Good question. Yes, so okay, so if you had an algorithm where as the input increases, the run time would decrease. It's a little odd because what you're saying is that as you're adding like bigger, like more and more data, somehow you have to run for less and less time. Now, it's possible that, I mean, usually the way the data is structured is not that you have, like, larger instances aren't generally speaking, you know, more qualitative information about your original set of data. So you know, that could decrease the runtime, usually it's kind of more instances of like unstructured data or data that you also have to work on. So it would be unusual to have an algorithm whose, as your input gets larger, as there's more data to work with, your runtime will actually decrease. I can't think of any off the top of my head, but it's, you know, I imagine there are some that are like that, but yeah. I suppose another way to kind of think is maybe there are some algorithms where for very small values of n, the runtime is just super inefficient, but then at some point it levels off and figures itself out. So those ones you don't really care about the initial parts, you only care about down the line. But generally speaking, something would be like one over two to the n is the runtime is very unusual and those don't really appear commonly. Yeah, good question. Okay, so efficient. So we say an algorithm is efficient if its run time is polynomial. So, Efficient if it's worst case run time is polynomial time. So here we have efficient, and this is kind of a theoretical computer science definition. So the reason why we use efficiency in this way-- so one, polynomial time is nice because if you take something polynomial time, you run it polynomial many times, it remains polynomial time. So it has these nice composability properties. Another useful thing is that if you scale the input size by, say, a factor of two. And the runtime also scales by a constant. But generally speaking, this works pretty well in practice in that most algorithms, if they have a polynomial-in-time algorithm, are like 5n squared or something, not 12 billion n to the 12 billion. So this definition kind of works well in practice. It's very nice to use. It's very composable. And that's what theoreticians call efficient, but in practice, do these constants matter? The constant exponent matters, the constant in front of there that's hidden by the big O also matters as well. Okay. But our goal in this class will be that all of your algorithms should be at least polynomial time. If your algorithm is running in exponential time, that's not good. if your algorithm is running in time more than this, that's not good for this class. So in practice and for this class, you want your algorithms to be at least polynomial time. Okay, any questions so far? - Okay, yeah. - I'm sorry, is tfn the run time, right? - Yeah. - So is that a, that's supposed to be a function and all the efficient run times, I guess, contain that function? - Yeah. - So what we're saying, okay, yeah. So tfn here is the run time as a function of the input length. So for example, if we took, okay, I like, we can imagine n is the input length here, but you could say maybe f takes in some value. Okay, that's a little, anyway, we could say, generally based on how big the input size is. And so for example, if you multiply numbers, usually in the class, it'll be small, but you can imagine multiplying bigger and bigger numbers, and then the runtime would scale with very large numbers. So yeah, T of n is the runtime, it's a function of its input length. And when we say it's efficient, we mean so there's some polynomial, the upper bounded. So maybe the upper band is n squared, maybe the upper band is n cubed, maybe it's n to the 20th. No matter what the polynomial is, if it is upper bounded with big O notation by a polynomial, then we say it's polynomial time and or efficient. for that answer to the question. Okay, so for this class, you'll pretty much only ever really be using big O notation except for kind of the homework, but I will cover because big O notation is an upper bound. And you know, okay, so one thing to note is that because it's an upper bound, if I say something is order of n squared, it's also order of n cubed because n cubed is like bigger than n squared. So when we were asking to give an upper bound for your algorithms, you could say maybe all of them are ordered 2 to the n, which would be correct because hopefully all your algorithms are upper bounded by 2 to the n, but that's not what we want. We want something that's like reasonably tight. So here in this thing, if you had said the runtime here is ordered 2 to the n, well that's technically correct because sure, 2 to the n is is much bigger than the time it takes to run this, that's not what we want. We saw something that's relatively close. Now, if on the homework or whatever, you give an answer, maybe the answer was supposed to be n squared if you do some nicer analysis, but you got n cubed, right? If it wasn't like as tight as possible, that's okay. We will let you know on exams and stuff when it matters, if we want it to be like tight or what the target runtime will be. So as long as it's something reasonable, reasonably close to kind of what we expect. That's good enough for, you know, homework and exams, unless we say otherwise. We don't, but we don't want you saying like the upper bounds to the n or something very, very large, which does not actually show us like understanding of what's happening here. Okay, so I'll just briefly kind of talk about the other types of bounds we'll use. So we have, you should think of them kind of as follows. So I'm going to write some sort of, so these are just kind of other bounds. So I'll write them kind of as a combined definition. And you can kind of see where the comparisons will come in. So we say that f of n is, and so an option from kind of, this way. So we have little o of g of n, we have big O of g of n, we have big Omega of g of n, and we have little Omega of g of n. Okay, so how do you read this? Basically, if you go to the top line, you stay at the top line. Oops. So I'm I'm writing four definitions at once, and you can kind of like, you know, it's going to branch out and you can stay at the top line. If you go to the top line, you can stay at the second line, just go to the second line. Okay. So you pick one of these, f of n is one of these values. We've talked about big O of n. If, and then there's going to be another branch. So big O of n, we've already talked about, this is if there exists. Okay, there's a question, I'll answer this after I define this. There exists, there exists, this is for all. So constants, c greater than zero. And we can say then there's been another branch. Yeah, so for all consts of C bigger than zero, we can say there exists an n0 bigger than equal to zero, such that for all large enough n, Okay, f of n, and here we're going to split. So little o is less than c times g of n. Big O is less than or equal to. Big Omega is greater than or equal to. little omega is greater than c times g of n. Okay, so let me explain how to read this. So, you basically pick a path. So you can say f of n is big O of g of n if there exists a constant c greater than zero and there exists an n zero greater than zero such that for all n, at least n zero, f of n is less than or equal to c times g of n. So that's one definition. Another definition you could say, take the bottom back, you could say f of n is little omega g of n if for all constants c greater than zero there exists n zero greater than x zero such that for all big enough n f of n is greater than c times g of n. So that's how you read this combined definition. And what we can see is that big O as expected corresponds to like less than or equal to and little o corresponds to basically strictly less than. Big omega corresponds to greater than or equal to, so this is a lower bound, and little omega is a strict greater than, so this is also kind of a lower bound. So yeah, so if you're trying to remember how do I remember what these are, these are kind of relatively good values because they're comparing f with g of n. So yeah, so just a little bit more on this definition. So here, we're just saying for all these definitions are talking about large values of n, once n gets big enough, F of n should be upper or lower bounded by some constant times g of n. And for these type ones here, this little o and little omega, what we wanna say is that, so when we say F of n is little omega of g of n, What we're saying is that f is always less than c times g of n, no matter what constant you have. So that's why we have the for all here. We say, no matter what c is, it's actually strictly less than that. Whereas when we say it's big O, we just say there's some constant that it's less than or equal to. And that's where those definitions come in. And then one last definition we say that f of n is theta of g of n if f of n is big O of g of n and if n is omega of g of n. So this is a tight bound. This means that f is less than or equal to kind of n greater than or equal to. So it's runtime is basically tightly bound by Juvent. Okay, let me go back to the question chat. So someone said whenever we consider big O time complexity, it's always in regard to runtime. So okay, so yeah, so big O notation can be used for any function. You can have f and g be any two functions. For this class, you will basically primarily be using it to say, A runtime is bounded by some function. So that's the usage in this class, but big O notation can be used in general for more things. Okay. Any other questions on this? Okay, so at this point, we're going to have, I'll do a 10 minute break until 11.05. And then we'll talk about how to analyze the efficiency of the Gale-Jabry algorithm. And then after that, we'll start talking about graph algorithms a little bit. So I'm gonna pause the recording during the break and then we can, yeah, just talk about that. Okay, so just a couple of things people asked about during the break that's not on the recording right now. Someone asked that if f of n is theta of g of n, is it equal to g of n? Not necessarily. So for example, five n squared is theta of n squared, but five n squared is not equal to n squared. And you should think of this theta, right? these notations, there's a constant here. And so what it means for something to be theta of something else is that they're kind of the same up to constant factors. So here, the five, if we ignore the five, then the numbers are the same. Here we can see six and squared is theta of two and squared plus two, because the two part is very small. And if we ignore the constants here, then they're about the same. So this means the growth rate of the functions is relatively similar. Also, when with little o and little omega, these kind of strict values, you want the growth rate to be much smaller. So 5n squared is not little o of n squared, but say 2n is little o of n squared. And this is because n squared outpaces 2n for large values of n, whereas 5n squared is, you know, related by a constant to this n squared. And so it's not that this value outpacing 5n squared by a large amount. So when you get little o which is the less than here, the little is less than, and little omega which is the greater than, you should think of these as the g of n is kind of far outpacing f of n in terms of either being larger or being smaller, and they're not kind of keeping pace. Okay, and yeah, these these functions as you expect, there's something with little o, of g of n is also big O of g of n. Okay. Someone asks, doesn't this mean we wouldn't, usually wouldn't use little O or little omega if g of n and f of n are the same order? Yes, so if f of n and g of n are theta, if they're the same order, then they are not little O and they're not little omega. So yeah, so if something is theta of g of n, it's also by definition big O and big omega, and it is also by definition not little o and not little omega. All right, so now let's talk about the Gale-Shapley algorithm. So here we're gonna analyze the Gale-Shapley algorithm. So let me pull this algorithm actually from notes from last time. All right. I'll fix those in a second. Okay. So recall what happened in the Gale-Shapley algorithm. We started out with an empty matching. And then for every hospital-- so we wanted to rematch hospitals to students, such that hospitals and students don't want to swap with each other based on their matchings. And how the algorithm works is that whenever a hospital is unmatched, they have a chance to give an offer to a student. They'll give offers to the students in decreasing order of who they like. But once they've made an offer and they've been rejected, they don't ask that student again. If the student is unmatched, then they'll say yes temporarily. And if they're already matched, then they'll look at their current partner versus their new offer. And then whichever one they like better, that's the one they'll stay with. So if they already like their old partner better, then they're going to say no to the new offer. And if they like the new offer better, then they'll get rid of their old offer and take up the new offer. And we do this until all the hospitals basically get matched, which we can prove with the number of hospital the same always happens. So last time we proved that this algorithm always outputs a stable matching. And now we want to analyze how long it takes to run this algorithm. OK. So when we're looking at this algorithm, and we're thinking, how long does it take to run? You might start by saying, OK, first of all, what are we using to represent the input and the output? That's always a good question. What data structures do we use? because sometimes that will matter. And then there's a lot of other stuff that happens, right? We say, well, a hospital is unmatched and hasn't been offered to every student. How do we know, how do we find a hospital that's unmatched? How do we determine if they made an offer to every student? Right? When we say S is the first student on H's list to whom they haven't made an offer yet, we have to ask, how do we determine that? So like, is this very easy to do? Or is this very hard to do? So naively, right, what we could do is imagine that, you know, to find the first student they haven't made an offer yet, you have to go through their entire list every time. So maybe you have a whole list of all their students and you put check marks by them. But that would actually be a little bit more inefficient. So when you see stuff like this, and if I said, take this program and code it in C++ or your favorite programming language, And you look at a task and you're like, "I don't really know how I want to do that," or "I have to spend some time thinking about how actually should I do this." That's when we want you to write for homework and exams a more clear time complexity analysis of how actually you plan to do that. And the goal is that what you write down will be sufficient that if you did want to implement this, you don't actually have to make any important decisions on how to actually do certain steps. Okay, so another thing too for this class, if say, you know, we have this Gale-Shapley algorithm, we show you the analysis, which we're about to do, and then the homework, you have, you maybe come up with an algorithm that's very similar, you don't have to prove everything from scratch. You can say the algorithm is similar to the Gale-Shapley algorithm except we do x, y, z, and then you can say this changes these proofs in this way. So you don't have to reprove things. You can just say what changes relative to what we've already done in class. That's also sufficient. For the exams, you're also allowed to refer to anything we did in class or on homework even, or even in a textbook. But just make sure that if you say, for example, on the exam, that as was proven in the textbook, and it's not proven in the textbook, right, that's going to be a problem. But if it is proven in the textbook or it was proven during class, it's totally fine for the exam, for the homework, for anything in this class to just say, as was proven in the class, run homework, or et cetera, because you don't need to reinvent the wheel. Yeah. Okay. So, oftentimes for exams, or for exams pretty much will give you the input and output format, but for homework, you're free to choose whatever you want that to use. So as long as kind of like, whatever you give me as an input and output is reasonable, that given that output, I could figure out what the matching is supposed to be. That's totally fine. You're given kind of freedom to choose your inputs and outputs as you wish. And again, your analysis does not have to necessarily be optimal as long as it's reasonable. And maybe you have some inefficiencies, that's okay. As long as it's reasonable, you'll get full credit for it. Okay, so let's talk about the input and output. So the input is these preference lists. So each hospital is gonna rank every student. Each student is going to rank every hospital. Any ideas for how we should represent the input like as a data structure? If I was to say, write down a preference list for hospital and students and put into code somehow, how would you put it into code? Like what data structure would you use to store this information? - This is a first guess, but would a stack be helpful here? - You could use a stack. Yeah, so it will turn out for this. It doesn't really matter too much what you're using for this. Someone said an array. So there's lots of different ways you could do this. We could do a stack. So yeah, let's do a stack. The notes kind of online will have a, I believe it has it as array. You could also do it as a linked list. But a stack is totally fine. And kind of maybe the reason why you might think, let's use a stack in this case, is actually you're thinking that we're trying to find, you know, unmatched, right, hospitals are trying to find their first student they haven't made an offer to, so you can maybe pop a student off the stack, which could be useful. So yeah, we'll go with a stack. And again, there are different ways you could do this. There's not necessarily one correct idea. So the preference list-- oops. We'll say that the preference list for-- will be a stack with sort of the most, the highest ranked students on top. Students who are hospital. Okay, so your input will basically then, in this case, be 2 n stacks. So the total input in this case will be 2 n stacks, which are parentheses. Okay. All right. What about the outputs? Any ideas? So the output should be a matching of some sort, or a matching instead of pairs. Someone said a dictionary hash net, or hash. Okay, yeah, that's a good idea. So yeah, so we kind of note that we're going to want to be keeping track of pairs, adding and removing pairs. So here, we're going to want to have a little bit more flexibility. And a lot of times, again, when you're doing this, maybe you'll start with something. You'll try something, and then later down the line, you'll be like, oh, there's something I'd want to do, and that's annoying to do that. And then you can change your mind. So as with analyzing algorithms, it's not uncommon to make decisions and then change your mind later if you find something more efficient. So for the output, I'm not quite going to do a hash map. I'm just going to do two arrays. So the output will be two size and arrays. And we're going to have hospital and students. And what will the hospital do? we'll say that hospital I equals J if, or so let's say hospital H is S, equals S if H is paired with S and maybe negative one otherwise. So we'll number the students in hospitals from zero to one, and we'll have a student array, which will be kind of the reverse. We'll say that student s equals h. Let me move this down. If s is paired with h. And negative one others. Okay, so just one of these would be sufficient to figure out what the matchings are, right? Could you go through the hospital array for every hospital and figure out who the students are matched to or if they're unmatched. The reason we're also keeping the student array is because it will make other operations more easy in the future. So that's what we're gonna have. And again, there are many different kind of options for this. This one here is one that was chosen for later things, but anything that's reasonable is okay. Okay, any questions on what the input and output format is? All right, so now we need to figure out how we're gonna do each of these other steps. So let's go look over here. And we wanna figure out when a hospital is unmatched. All right. Oh yeah, and then these will have to be initialized. So we'll do that a little bit later, but I guess we can do that. So we'll say initially we'll do that later actually. So any ideas how we might find an unmatched hospital? Who hasn't made an offer to students? If you were just to do this naively based on what we have so far. We have two arrays that keep track of the matches, and we have these stacks of preference lists. Yeah, so you can check if hospitalH equals negative one. So basically, one way to do this, if you're trying to find an unmatched hospital, you can go to the hospital array and see if you find a negative one. And if you find a negative one, then that's an unmatched hospital. You can proceed from there. And then you can check if there's any students left on their students stack. And if so, then there's a student to make an offer to. So the time it will take to do that-- so this is valid as a solution. You could just say, whenever we start running this while loop, we'll just go through our entire hospital array until we find a hospital that's unmatched, and then we'll go work with that. Now, it turns out you can have more efficiency. So I will show you a more efficient way to do this. But what was just described, kind of just go through this whole array, see if one of them's negative 1, is a valid solution and would work. But there's kind of a more efficient way to do this. OK. So I'm just going to create a separate queue or stack for keeping track of unmatched hospitals. So how to find unmatched hospitals. So we're going to have a queue. AQ called, I'll call it free hospitals, so hospitals that are unmatched. Where H is in free Q, if H is unmatched. And also, it's not meant to offer to every student. Okay, so I'm going to initialize a new data structure. And this data structure will call free hospitals. And I'll just keep all the hospitals there that are unmatched. So initially, initially, we'll have free hospitals contains all hospitals. hospitals. And then to do this step, to find an unmatched hospital, we simply have to pop H off the queue. So I don't want to go through this array every time, so I'm just going to "Well, I know I'm going to need free hospitals." And once a hospital, I'll just pop it off the queue. And then if it becomes, well, hospital becomes free, I'll add it back onto the queue. So this is an optimization you could do. Again, if you had an answer, if you didn't do this optimization, that's not wrong. It just will mean that your runtime will take longer. Any questions about this so far? By free Q do you mean free hospitals? Because you're a free hospital where H is in free Q. I'm calling the Q free hospitals. This is the name of the Q. Because then this is where right next right next to it says where H is in free Q. What is free Q? Oh, sorry. Yeah, this would be free hospitals. Yeah. Thank you. Yeah, okay. All right, so now we have a way to find an unmatched hospital. Now we need to figure out where the first student on the list to whom they haven't yet made an offer. And this is where it's kind of nice that we're storing the preference list as stacks. So what we can do is we can just pop the top student off of the hospital preference list. And if we never push it back on later, then we can find them. So let me write this down here. So how to find next student to make an offer to. And offer two. This is pretty simple. We'll just pop the next student off of H's preference list stack. And this is fine because the stack stores them in a decreasing preference order. So yeah, this is a nice advantage of the kind of choice Data structure here. And if it's empty, right, then, well, it shouldn't be empty. We kind of proved that last time that you won't get to a case where it's empty, but it shouldn't be empty. You could just kind of break out of this early, or maybe H shouldn't be in this queue. Okay. The next task here, we need to check if S is unmatched. And if so, we need to add matching. So now this is why we've gone to the student-- why we have the student list. The student array allows us to easily check if s is a match by simply looking at the student array here. We can very easily tell what is student s. We can say, oh, are they matched? And if so, who are they matched with? So that's where that thing will come in. So how to find whether S is matched, and if so, to whom? Here we can just consult the student array data structure. So this is the student array. So that's why we're keeping that. Also, when I'm writing it a little bit maybe more thorough than what you would need to write, you can kind of have these as, you can maybe talk about your data structures in a list, and then have comments up here. But I'm doing this more slowly since it's the first time you've kind of seen this analysis. All right, what do we need to do next? Well, we're going to, we've been able to find whether they're matched, if so, who they're matched to. And now we're going to need to do two things. One is we're going to have to check whether s prefers h to h prime. That's going to be kind of hard. Another thing, we're going to need to add or remove matchings. So let's start with how do you add and remove matchings? So let's say I wanted to add H comma S to M. Given our data structures, how would you do that? So keep in mind the output here is keeping track of the matchings. So how would I indicate that H comma S are now matched together, if I just wanted to do this step? We just changed the hospital age to S and student S to H. Yeah, exactly. We just update these accordingly. So, to how to add or remove matches. So to add HS, what you do is you set hospital H equals S and student S equals H. And then to remove HS from SM, what you'll do is you'll set hospital H to be negative one, and you could set student S to be negative one as well. And we're also going to wanna make sure we're updating the list of free hospitals. So we'll set hospital H to be negative one and add age to the free hospital queue. Just so that's updated, 'cause now they're unmatched again. All right, any questions so far on this? But yeah, all we're doing in Bridges, we have these data structures. So there's ways, with the way we've set them up, it's actually very easy to add or remove hospitals and students from the list. Okay, so now we have the last thing we need to do. which is we need to compare the preferences of a student from one hospital to another hospital, where these could be like arbitrary random hospitals in the preference list. And currently the student preference list is being stored as a stack. So a naive way to do this would be to basically look through everything in the stack and see which one you get to first, and then you might have to push them back on after you take them off. So again, there could be a valid solution Or you can imagine, this is the student stack. You start kind of moving stuff over to another stack until you find either h or h prime. And then once you find it, you have to restore it. So you move the stuff back. So that's kind of one way you could do it. It would take some time. But it would be like a valid solution. But we want something a little bit more efficient. So any ideas maybe on what kind of data structure maybe to use if we want for an efficient lookup in a sense? Someone said a hash row? Okay, yeah, so you're saying, so basically idea, right? you want to have, be able to sort of look up sort of the student preferences or thinking what has efficient lookup. And a HashMap does have efficient lookup. So that's a good idea. So what we'll actually use here is an array, which does have efficient lookup as well. But there's possibly, you could have some sort of HashMap based solution. And again, like I said, some of these optimized ways we're making are not, you wouldn't necessarily have to make all these optimizations on a homework problem or something. It's just kind of showing that you can optimize a lot. Okay, so what I want is I want to be able to take S, H and H prime and exactly determine very quickly, hopefully in constant time, whether or not S prefers H or H prime. So the way I'm gonna do this is I'm going to, at the very beginning, take some time and initialize sort of a more exact student preference, like a better data structure to store student preferences for allowing this to happen. So let me talk about this. So how to compare S's preference, oops. Whether S likes H or H prime better. So I'm going to construct another data structure, which I'll call student_pref. So for each student, each student s. At the beginning, we'll construct a size n array, which we'll call student_prep. We'll make this a size-- I'll say-- OK, we'll make it a size n by n. So construct a size n by n array, where student pref s of h is the ranking of h in the preference list of s. So for example, if we have student pref sh equal 2, means h is ranked second on s's preferences. All right, so what we noticed was that our data structure for storing student preferences was not allowing for a nice lookup. So at the beginning, we're going to transform it into a better data structure to allow this lookup. And then to compare, you simply will compare student_pref_sh to student_pref_sh_prime. sh to student_pref. All right, so again, what should you think about this? We're just doing a transformation. We said stack or linked list also. They're not very good for allowing this little easy lookup. So at the beginning, we'll take some extra time, but it's a one-time cost because once we have the data structure, we're okay. We do a one-time cost at the beginning where we transform into a better data structure allowing for easy comparisons between student preferences, which is what we actually need. Okay, questions on this? All right. So now let's actually do the whole analysis. We've talked about kind of how we're going to do all the steps. If we look back here, we have figured out how to do this task. We have our input. We have our output. out how to find unmatched hospitals. That's through the free hospital queue. We figured out how to find the next student, which is just to pop them off the hospital preference list. We figured out how to find students, whether they're matched and who their current partner is, based on their outputs. And then we changed the preference list for students to allow easy comparison. And then removing, adding, kind of the same as before. And that's what we're going to do. So let me kind of go through and talk about now the final kind of analysis. So we'll copy this down. Okay. So let me move this down. OK, so part one we're going to set and be the MC set, but more generally we're going to kind of initialize. Our data structures. Our data structures. So what are our data structures? We have the student list. We have a hospital list or array. We have the free hospital queue. And we have student prep. OK, so student, if you recall, was a size and array. And you set all values to negative 1. Hospital was also a size and array, and originally everything's unmatched. Set all values to negative one. Pre-hospital, you add all hospitals to the queue. And for student_prep, this is an n by n array. And you're going to initialize by going through all student prefs. So basically, you'll pop the first student off, and that'll be rank one, so you'll set s comma h equal one. Yeah, so someone asked, do we have a hospital pref? So we don't have a separate hospital pref. So the preference list here for the hospitals, this will be stacks. So there's, this is kind of like, these are n stacks here, and there's n more stacks here. So hospital prep, it's okay for us to leave as a stack, because that's sufficient for what we need. For student prep, we wanted to transform it into this kind of two n by n array, because the stack version wasn't very good for what we wanted. Okay, so how long do each of these steps take? So how long does it take to initialize student, hospital, free hospital, student prep? Or each one, one by one? Someone says, can you repeat what comes after initialize and before going in the student prep initialization? Okay, yeah, so what happens here? So how do you initialize each of these student prep arrays? So I'll say, let's say student one or student zero. I'll pop possible h off, and let's say h is five. And I'll set student pref s comma 5 equal to 1, because I know it's the first rank. Then I'll pop the next student in the hospital off. Let's say it's h prime. And I'll say student pref s h prime is equal to 2. And then I'll pop the third hospital off, and I'll set student pref s comma h double prime, let's say to three. So you can basically initialize this by popping every hospital off of every student stack and just assigning the values as they get popped off. OK. Yeah, good question. All right, so how long does it take to initialize this student array here as a function of n? - N? - O of n, yeah. So you'll just create an array and it's at all values to negative one takes time one for every value of the array. All right, what about hospital? - Same, O of n. >> Okay, and then pre-hospital and student prep? >> O of N. >> Okay, so this one's O of N, just you just push every hospital to Q. And then what about student prep? >> Would that be N squared? >> That'd be N squared, yeah. So essentially what you'll be doing is you'll go through every hospital and every student preference list and there's N squared total of them. So we'll take, or another way to think about it, you have an N by N array, you want to fill every spot, because that will also take n squared time. Now let's go to the next part. So to find an unmatched hospital, how do you do this? We're going to pop-- I call these notes. Add some more space here. Here we're going to pop a hospital off of Rehospital. All right. Then to find a student they haven't made an offer yet, Here we're going to pop s off of h's preference list. Stack. Okay, and then, let's just start with these. So how long does this update by itself take? >> This is all of them? >> So I mean, just one iteration, in one iteration, how long does this take? >> All of them just one? >> Yeah, it's just this is cost of time per iteration. And then how about this step down here, where we're popping this student off? >> Same. >> Same, yeah. So here, so yeah, it's perfectly valid. my homework and stuff to basically add comments for the runtime. Kind of like in this style. I'm being even more, I'm being very specific in how I do this, but this is totally a valid way to do this stuff. All right, again, how do we find if s is a match? We're going to check student of s. to see who HS is matched with. And then to add H to S or remove this, so these three steps here, these are done by updating hospital and student. and maybe free hospital. So as we mentioned before, to do adds and removes, you just set hospital age and student as to whatever value it needs to be. And then if the hospital becomes free, you have to add them back to three hospital. Okay, and then this step here, we said, And we compare student pref sh versus student pref sh prime. And then to reject the offer here, all we have to do is add h back to the hospital queue. Since we're not actually changing the matches. Okay, so I'm just summarizing what we already said, how we're doing this. So how long does it take to check? How long do each of these steps take, I guess? Any ideas? >> 01, aren't they constant? >> Yeah, so it turns out all this will be 01. So to check student S, this is an array lookup. That's just O1 per iteration. This comparison, it's O1 lookup and O1 lookup, and then a single comparison. So this is also O1 per iteration. Adding and removing, so this is an array lookup and change, array lookup and change, and adding to a queue. This is O1 per iteration. And then adding H back to the free hospital queue is O1 per iteration, and returning m is also O1 time. Okay, and then how many iterations do we have? We had a proof about this last time. Anyone recalls what our upper bound on the number of iterations? >> Would it be n squared? >> Yeah. So here we have O of n squared iterations. This is from last time. The way you can think about this is that every hospital can make an offer to each student at most once. So every hospital can make at most n offers, And there's n hospitals and students, which is n squared total offers maximum. Now, it turns out this actually ends up being relatively tight as an analysis. There are some preference lists that will actually require roughly that many offers. But we're not going to prove that. But this was at least an upper bound on the number iterations. OK. So now to do the total runtime, all we have to do is add everything. So the total will be, so we have set up, which is n plus n plus n squared. This is O of n plus O of n plus O of n plus O of n squared. And this will end up being O of n squared. When you add things, it's upper bounded by the biggest one. And then we're going to do a plus the while loop, which is O of n squared iterations times O of one per iteration. And when you multiply, you just have to multiply the total times. So this is O of n squared. And when you add O of n squared plus O of n squared, then the total is O of n squared. So yeah, so this is, so yeah, when you do, we're adding bits. So yeah, so what happened here, I just said the total setup time is a bunch of things which are upper bounded by O of n squared. And then the while loop, all the stuff inside the while loop takes constant time, and we're doing n squared iterations. So n squared times constant is O of n squared. and O of n squared plus O of n squared is also O of n squared. So these are using the properties we showed earlier, where when you add, it's upper bounded by the addition. When you multiply, you multiply the times. OK. So yeah, so I guess this was a little bit more in-depth than maybe what you'd need. So if you were doing this for a homework problem, what would you probably want to do? So if you're going to have a bunch of helper data structures, I would recommend at least explaining somewhere, for the sake of the grader and to get partial credit and stuff, what the data structures are supposed to do. And then after you do that, having what I wrote here, where you just add comments, say exactly what you're doing, I can add comments here and then this would be fine. It doesn't have to. This long kind of track what's happening. Yeah. And this is kind of sufficient because hopefully, right, and you guys could do this at practice. If you went home, you could in C++ actually implement these. You know, I'll assume you have library access to like queues and arrays and all that stuff. you don't have to implement those in the class, but assuming you have like basic library functions for some nice data structures, you should actually be able to implement this at home. And, you know, each of the steps we kind of explained how to do this. Now on the homework, and again, if you have a problem similar to this, and all you do is say, we change this step or something, then it's enough to say, we're gonna use the Gale Shapely algorithm, except we're gonna change step blah, blah, blah to something else. The change in the runtime analysis is that this becomes more expensive, so the change becomes this. And the change in the proof is that we change this part of this specific proof, and thus we get this thing happening. So again, if you do something similar to what we've done in class, you don't have to do things from scratch, you just have to explain what you change and how that affects the time complexity analysis, and how that affects your proofs. Okay. So I think we're out of time, so I'm gonna end the recording here, And then I have office hours right now until 1 p.m. And yeah, thanks everyone. - Thank you.